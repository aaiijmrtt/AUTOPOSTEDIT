[global]
vocabsize = 10
wvecsize = 20
timesize = 5
batchsize = 5
depth = 4
iterations = 1
frequency = 1
logs = dumps/
path = dumps/model

[embedder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}

[encoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[bicoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[thinker]
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
_steps_ = ${global:timesize}
batch = ${global:batchsize}
nonlinear = relu6

[decoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[atcoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
memory = ${global:timesize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer
