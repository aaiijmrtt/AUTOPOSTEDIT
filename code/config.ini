[global]
vocabsize = 10
wvecsize = 20
timesize = 6
batchsize = 5
depth = 4
iterations = 1
frequency = 1
predictions = 5
align = True
load = dumps/load/model
logs = dumps/logs
save = dumps/save/model
output = dumps/output

[embedder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}

[encoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[bicoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[thinker]
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
_steps_ = ${global:timesize}
batch = ${global:batchsize}
nonlinear = relu6

[decoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
predictions = ${global:predictions}
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[atcoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
memory = ${global:timesize}
predictions = ${global:predictions}
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[fatcoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = ${global:depth}
_depth_ = ${global:depth}
steps = ${global:timesize}
batch = ${global:batchsize}
memory = ${global:timesize}
predictions = ${global:predictions}
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer
