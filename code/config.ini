[global]
vocabsize = 17702
wvecsize = 20
kvecsize = 20
timesize = 36
batchsize = 10
iterations = 1
frequency = 100
logs = dumps/
path = dumps/model

[embedder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}

[encoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[bicoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[thinker]
wvec = ${global:wvecsize}
kvec = ${global:kvecsize}
depth = 4
_depth_ = 4
_steps_ = ${global:timesize}
batch = ${global:batchsize}
nonlinear = relu6

[knowler]
kvec = ${global:kvecsize}
batch = ${global:batchsize}

[decoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
_depth_ = 4
steps = ${global:timesize}
batch = ${global:batchsize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[atcoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
_depth_ = 4
steps = ${global:timesize}
batch = ${global:batchsize}
memory = ${global:timesize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer
