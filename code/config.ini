[global]
vocabsize = 20000
wvecsize = 200
timesize = 50
batchsize = 100
iterations = 10
frequency = 1000
logs = dumps/
path = dumps/model

[encoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[bicoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[decoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer

[atcoder]
vocab = ${global:vocabsize}
wvec = ${global:wvecsize}
depth = 4
steps = ${global:timesize}
batch = ${global:batchsize}
memory = ${global:timesize}
predictions = 5
biencoder = True
lrate = 1e-2
dstep = 1000
drate = 0.9
optim = AdamOptimizer
